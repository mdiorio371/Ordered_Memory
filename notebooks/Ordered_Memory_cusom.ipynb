{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ordered_Memory_cusom.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-36nUdmoZ0S",
        "colab_type": "text"
      },
      "source": [
        "Model is adapted from https://github.com/yikangshen/Ordered-Memory \n",
        "By Shen et al. (2019)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBX3SpJZoWhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utakmGvRpSNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/yikangshen/Ordered-Memory.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud-BokZdgi4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('path/to/Ordered-Memory')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uxsb_umqhJdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Model set up (from By Shen et al. (2019))\n",
        "import argparse\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "import ordered_memory\n",
        "from utils.hinton import plot\n",
        "from utils.listops_data import load_data_and_embeddings, LABEL_MAP, PADDING_TOKEN, get_batch\n",
        "from utils.utils import build_tree, char2tree, evalb\n",
        "\n",
        "\n",
        "class ListOpsModel(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(ListOpsModel, self).__init__()\n",
        "\n",
        "        self.args = args\n",
        "        self.padding_idx = args.padding_idx\n",
        "        self.embedding = nn.Embedding(args.ntoken, args.ninp,\n",
        "                                      padding_idx=self.padding_idx)\n",
        "\n",
        "        self.encoder = ordered_memory.OrderedMemory(args.ninp, args.nhid, args.nslot,\n",
        "                                                    dropout=args.dropout, dropoutm=args.dropoutm,\n",
        "                                                    bidirection=args.bidirection)\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Dropout(args.dropouto),\n",
        "            nn.Linear(args.nhid * 2 if args.bidirection else args.nhid, args.nout),\n",
        "        )\n",
        "\n",
        "        self.drop_input = nn.Dropout(args.dropouti)\n",
        "        self.drop_output = nn.Dropout(args.dropouto)\n",
        "        self.cost = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input):\n",
        "        mask = (input != self.padding_idx).bool()\n",
        "\n",
        "        emb = self.embedding(input)\n",
        "        emb.transpose_(0, 1)\n",
        "\n",
        "        mask.transpose_(0, 1)\n",
        "        emb = self.drop_input(emb)\n",
        "        output = self.encoder(emb, mask, output_last=True)\n",
        "        output = self.mlp(output)\n",
        "        return output\n",
        "\n",
        "    def set_pretrained_embeddings(self, ext_embeddings, ext_word_to_index, word_to_index, finetune=False):\n",
        "        assert hasattr(self, 'embedding')\n",
        "        embeddings = self.embedding.weight.data.cpu().numpy()\n",
        "        for word, index in word_to_index.items():\n",
        "            if word in ext_word_to_index:\n",
        "                embeddings[index] = ext_embeddings[ext_word_to_index[word]]\n",
        "        embeddings = torch.from_numpy(embeddings).to(self.embedding.weight.device)\n",
        "        self.embedding.weight.data.set_(embeddings)\n",
        "        self.embedding.weight.requires_grad = finetune\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS9XgzhbhJZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## saving and loading (from By Shen et al. (2019))\n",
        "def model_save(fn):\n",
        "    if args.philly:\n",
        "        fn = os.path.join(os.environ['PT_OUTPUT_DIR'], fn)\n",
        "    with open(fn, 'wb') as f:\n",
        "        # torch.save([model, optimizer], f)\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': test_loss\n",
        "        }, f)\n",
        "\n",
        "\n",
        "def model_load(fn):\n",
        "    global model, optimizer\n",
        "    if args.philly:\n",
        "        fn = os.path.join(os.environ['PT_OUTPUT_DIR'], fn)\n",
        "    with open(fn, 'rb') as f:\n",
        "        checkpoint = torch.load(f)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        epoch = checkpoint['epoch']\n",
        "        test_loss = checkpoint['loss']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZBoo8dohJVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################################################\n",
        "# Training code (from By Shen et al. (2019))\n",
        "###############################################################################\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(data_iter):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_datapoints = 0\n",
        "    for batch, data in enumerate(data_iter):\n",
        "        batch_data = get_batch(data)\n",
        "        X_batch, transitions_batch, y_batch, num_transitions_batch, train_ids = batch_data\n",
        "\n",
        "        X_batch = torch.from_numpy(X_batch).long().to('cuda' if args.cuda else 'cpu')\n",
        "        y_batch = torch.from_numpy(y_batch).long().to('cuda' if args.cuda else 'cpu')\n",
        "\n",
        "        lin_output = model(X_batch)\n",
        "        count = y_batch.shape[0]\n",
        "        total_loss += torch.sum(\n",
        "            torch.argmax(lin_output, dim=1) == y_batch\n",
        "        ).float().data\n",
        "        total_datapoints += count\n",
        "\n",
        "    return total_loss.item() / total_datapoints\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    start_time = time.time()\n",
        "    for batch, data in enumerate(training_data_iter):\n",
        "        # print(data)\n",
        "        # batch_data = get_batch(next(training_data_iter))\n",
        "        data, n_batches = data\n",
        "        batch_data = get_batch(data)\n",
        "        X_batch, transitions_batch, y_batch, num_transitions_batch, train_ids = batch_data\n",
        "\n",
        "        X_batch = torch.from_numpy(X_batch).long().to('cuda' if args.cuda else 'cpu')\n",
        "        y_batch = torch.from_numpy(y_batch).long().to('cuda' if args.cuda else 'cpu')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        lin_output = model(X_batch)\n",
        "        loss = model.cost(lin_output, y_batch)\n",
        "        acc = torch.mean(\n",
        "            (torch.argmax(lin_output, dim=1) == y_batch).float())\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        if args.clip:\n",
        "            torch.nn.utils.clip_grad_norm_(params, args.clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.detach().data\n",
        "        total_acc += acc.detach().data\n",
        "        if batch % args.log_interval == 0 and batch > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print(\n",
        "                '| epoch {:3d} '\n",
        "                '| {:5d}/ {:5d} batches '\n",
        "                '| lr {:05.5f} | ms/batch {:5.2f} '\n",
        "                '| loss {:5.2f} | acc {:0.2f}'.format(\n",
        "                    epoch,\n",
        "                    batch,\n",
        "                    n_batches,\n",
        "                    optimizer.param_groups[0]['lr'],\n",
        "                    elapsed * 1000 / args.log_interval,\n",
        "                    total_loss.item() / args.log_interval,\n",
        "                    total_acc.item() / args.log_interval))\n",
        "            total_loss = 0\n",
        "            total_acc = 0\n",
        "            start_time = time.time()\n",
        "        ###\n",
        "        batch += 1\n",
        "        if batch >= n_batches:\n",
        "            break\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_parse(data_iter):\n",
        "    model.eval()\n",
        "    np.set_printoptions(precision=2, suppress=True, linewidth=5000, formatter={'float': '{: 0.2f}'.format})\n",
        "    pred_tree_list = []\n",
        "    targ_tree_list = []\n",
        "    crop_count = 0\n",
        "    total_count = 0\n",
        "    for batch, data in enumerate(data_iter):\n",
        "        sents = data['tokens']\n",
        "        X = np.array([vocabulary[t] for t in data['tokens']])\n",
        "        # if len(sents) > 100:      # In case Evalb fail to process very long sequences\n",
        "        #     continue\n",
        "\n",
        "        X_batch = torch.from_numpy(X).long().to('cuda' if args.cuda else 'cpu')\n",
        "\n",
        "        model(X_batch[None, :])\n",
        "        probs = model.encoder.probs\n",
        "        distance = torch.argmax(probs, dim=-1)\n",
        "        distance[0] = args.nslot\n",
        "\n",
        "        total_count += 1\n",
        "        depth = distance[:, 0]\n",
        "        probs_k = probs[:, 0, :].data.cpu().numpy()\n",
        "\n",
        "        try:\n",
        "            parse_tree = build_tree(depth, sents)\n",
        "            sen_tree = char2tree(data['sentence'].split())\n",
        "        except:\n",
        "            crop_count += 1\n",
        "            print('Unbalanced datapoint!')\n",
        "            continue\n",
        "\n",
        "        pred_tree_list.append(parse_tree)\n",
        "        targ_tree_list.append(sen_tree)\n",
        "\n",
        "        if batch % 100 > 0:\n",
        "            continue\n",
        "        print(batch)\n",
        "        for i in range(len(sents)):\n",
        "            if sents[i] == '<pad>':\n",
        "                break\n",
        "            print('%20s\\t%2.2f\\t%s' % (sents[i], depth[i], plot(probs_k[i], 1)))\n",
        "        print(parse_tree)\n",
        "        print(sen_tree)\n",
        "        print()\n",
        "\n",
        "    print('Cropped: %d, Total: %d' % (crop_count, total_count))\n",
        "    evalb(pred_tree_list, targ_tree_list, evalb_path=\"../EVALB\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description='')\n",
        "\n",
        "    parser.add_argument('--data', type=str, default='./data/listops',\n",
        "                        help='location of the data corpus')\n",
        "    parser.add_argument('--bidirection', action='store_true',\n",
        "                        help='use bidirection model')\n",
        "    parser.add_argument('--seq_len', type=int, default=100,\n",
        "                        help='max sequence length')\n",
        "    parser.add_argument('--seq_len_test', type=int, default=1000,\n",
        "                        help='max sequence length')\n",
        "    parser.add_argument('--no-smart-batching', action='store_true',  # reverse\n",
        "                        help='batch based on length')\n",
        "    parser.add_argument('--no-use_peano', action='store_true',\n",
        "                        help='batch based on length')\n",
        "    parser.add_argument('--emsize', type=int, default=128,\n",
        "                        help='size of word embeddings')\n",
        "    parser.add_argument('--nhid', type=int, default=128,\n",
        "                        help='number of hidden units per layer')\n",
        "    parser.add_argument('--nslot', type=int, default=21,\n",
        "                        help='number of memory slots')\n",
        "    parser.add_argument('--lr', type=float, default=0.001,\n",
        "                        help='initial learning rate')\n",
        "    parser.add_argument('--clip', type=float, default=1.,\n",
        "                        help='gradient clipping')\n",
        "    parser.add_argument('--epochs', type=int, default=50,\n",
        "                        help='upper epoch limit')\n",
        "    parser.add_argument('--batch_size', type=int, default=128, metavar='N',\n",
        "                        help='batch size')\n",
        "    parser.add_argument('--batch_size_test', type=int, default=128, metavar='N',\n",
        "                        help='batch size')\n",
        "    parser.add_argument('--dropout', type=float, default=0.1,\n",
        "                        help='dropout applied to layers (0 = no dropout)')\n",
        "    parser.add_argument('--dropoutm', type=float, default=0.3,\n",
        "                        help='dropout applied to memory (0 = no dropout)')\n",
        "    parser.add_argument('--dropouti', type=float, default=0.1,\n",
        "                        help='dropout for input embedding layers (0 = no dropout)')\n",
        "    parser.add_argument('--dropouto', type=float, default=0.2,\n",
        "                        help='dropout applied to layers (0 = no dropout)')\n",
        "    parser.add_argument('--seed', type=int, default=1111,\n",
        "                        help='random seed')\n",
        "    parser.add_argument('--cuda', action='store_true',\n",
        "                        help='use CUDA')\n",
        "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
        "                        help='report interval')\n",
        "    parser.add_argument('--test-only', action='store_true',\n",
        "                        help='Test only')\n",
        "\n",
        "    randomhash = ''.join(str(time.time()).split('.'))\n",
        "    parser.add_argument('--name', type=str, default=randomhash + '.pt',\n",
        "                        help='exp name')\n",
        "    parser.add_argument('--wdecay', type=float, default=1.2e-6,\n",
        "                        help='weight decay applied to all weights')\n",
        "    parser.add_argument('--std', action='store_true',\n",
        "                        help='use standard LSTM')\n",
        "    parser.add_argument('--philly', action='store_true',\n",
        "                        help='Use philly cluster')\n",
        "    args = parser.parse_args('')\n",
        "\n",
        "    args.smart_batching = not args.no_smart_batching\n",
        "    args.use_peano = not args.no_use_peano\n",
        "\n",
        "    # Set the random seed manually for reproducibility.\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        if not args.cuda:\n",
        "            print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "        else:\n",
        "            torch.cuda.manual_seed(args.seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwJRgVMBhJNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args.cuda = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QF5ydREf0nH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "    ###############################################################################\n",
        "    # Load data and run model (from By Shen et al. (2019))\n",
        "    ###############################################################################\n",
        "    train_data_path = os.path.join(args.data, 'train_d20s.tsv')\n",
        "    test_data_path = os.path.join(args.data, 'test_d20s.tsv')\n",
        "    vocabulary, initial_embeddings, training_data_iter, eval_iterator, training_data_length, raw_eval_data \\\n",
        "        = load_data_and_embeddings(args, train_data_path, test_data_path)\n",
        "    dictionary = {}\n",
        "    for k, v in vocabulary.items():\n",
        "        dictionary[v] = k\n",
        "    # make iterator for splits\n",
        "    vocab_size = len(vocabulary)\n",
        "    num_classes = len(set(LABEL_MAP.values()))\n",
        "    args.__dict__.update({'ntoken': vocab_size,\n",
        "                          'ninp': args.emsize,\n",
        "                          'nout': num_classes,\n",
        "                          'padding_idx': vocabulary[PADDING_TOKEN]})\n",
        "\n",
        "    model = ListOpsModel(args)\n",
        "\n",
        "    if args.cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    params = list(model.parameters())\n",
        "    total_params = sum(x.size()[0] * x.size()[1]\n",
        "                       if len(x.size()) > 1 else x.size()[0]\n",
        "                       for x in params if x.size())\n",
        "    total_params_sanity = sum(np.prod(x.size()) for x in model.parameters())\n",
        "    assert total_params == total_params_sanity\n",
        "    print(\"TOTAL PARAMS: %d\" % sum(np.prod(x.size()) for x in model.parameters()))\n",
        "    print('Args:', args)\n",
        "    print('Model total parameters:', total_params)\n",
        "\n",
        "    # Ensure the optimizer is optimizing params, which includes both the model's weights as well as the criterion's weight (i.e. Adaptive Softmax)\n",
        "    optimizer = torch.optim.Adam(params,\n",
        "                                 lr=args.lr,\n",
        "                                 betas=(0, 0.999),\n",
        "                                 eps=1e-9,\n",
        "                                 weight_decay=args.wdecay)\n",
        "\n",
        "    if not args.test_only:\n",
        "        # Loop over epochs.\n",
        "        lr = args.lr\n",
        "        stored_loss = 0.\n",
        "\n",
        "        # At any point you can hit Ctrl + C to break out of training early.\n",
        "        try:\n",
        "            scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'max', 0.5, patience=2, threshold=0)\n",
        "            for epoch in range(1, args.epochs + 1):\n",
        "                epoch_start_time = time.time()\n",
        "                train()\n",
        "                test_loss = evaluate(eval_iterator)\n",
        "\n",
        "                print('-' * 89)\n",
        "                print(\n",
        "                    '| end of epoch {:3d} '\n",
        "                    '| time: {:5.2f}s '\n",
        "                    '| test acc: {:.4f} '\n",
        "                    '|\\n'.format(\n",
        "                        epoch,\n",
        "                        (time.time() - epoch_start_time),\n",
        "                        test_loss\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                if test_loss > stored_loss:\n",
        "                    model_save(args.name)\n",
        "                    print('Saving model (new best validation)')\n",
        "                    stored_loss = test_loss\n",
        "                print('-' * 89)\n",
        "\n",
        "                scheduler.step(test_loss)\n",
        "        except KeyboardInterrupt:\n",
        "            print('-' * 89)\n",
        "            print('Exiting from training early')\n",
        "\n",
        "    model_load(args.name)\n",
        "    generate_parse(raw_eval_data)\n",
        "    test_loss = evaluate(eval_iterator)\n",
        "    data = {'args': args.__dict__,\n",
        "            'parameters': total_params,\n",
        "            'test_acc': test_loss}\n",
        "    print('-' * 89)\n",
        "    print(\n",
        "        '| test acc: {:.4f} '\n",
        "        '|\\n'.format(\n",
        "            test_loss\n",
        "        )\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_kaeYiMpsow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZiFbz3rJhbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}